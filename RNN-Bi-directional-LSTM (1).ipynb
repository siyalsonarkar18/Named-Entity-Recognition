{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING TRAIN TEST VALIDATION\n",
    "import re\n",
    "def read_data(f_name):\n",
    "    tweet_word=[]\n",
    "    tweet_tag=[]\n",
    "    t_word = []\n",
    "    t_tag = []\n",
    "    for line in open(f_name, encoding='utf-8'):\n",
    "        line =line.strip()\n",
    "        if not line:\n",
    "            if tweet_word:\n",
    "                t_word.append(tweet_word)\n",
    "                t_tag.append(tweet_tag)\n",
    "            tweet_word = []\n",
    "            tweet_tag = []\n",
    "        else:            \n",
    "            word,tag = line.split()\n",
    "            #print(word)\n",
    "            URL_pattern= re.compile(r\"http\\S+\")  \n",
    "            USR_pattern = re.compile(r\"@\\S+\")\n",
    "            word = URL_pattern.sub('<URL>',word)\n",
    "            word = USR_pattern.sub('<USR>',word)\n",
    "            #''.join([USR_pattern.sub('<USR>',w) for w in word])\n",
    "\n",
    "            tweet_word.append(word)\n",
    "            tweet_tag.append(tag)\n",
    "    return t_word,t_tag\n",
    "\n",
    "train_word, train_tag = read_data('data/train.txt')\n",
    "validation_word, validation_tag = read_data('data/validation.txt')           \n",
    "test_word, test_tag = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    tok_ind_list = defaultdict(lambda:0)\n",
    "    ind_list = []\n",
    "    for w in special_tokens:\n",
    "        ind_list.append(w)\n",
    "        tok_ind_list[w]= len(ind_list)-1\n",
    "    for ind in range(len(tokens_or_tags)):\n",
    "        for w in tokens_or_tags[ind]:\n",
    "            if(w not in tok_ind_list):\n",
    "                ind_list.append(w)\n",
    "                tok_ind_list[w] = len(ind_list)-1\n",
    "                \n",
    "    return tok_ind_list,ind_list\n",
    "            \n",
    "special_word = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token_dic, token_ind = build_dict(train_word + validation_word, special_word)\n",
    "tag_dic, tag_ind = build_dict(train_tag, special_tags)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token_dic[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag_dic[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [token_ind[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [tag_ind[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token_dic['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag_dic['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class BiLSTMModel():\n",
    "    pass\n",
    "\n",
    "\n",
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
    "  \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate')\n",
    "    \n",
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix,dtype = tf.float32)\n",
    "    \n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units = n_hidden_rnn)\n",
    "    backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units = n_hidden_rnn)\n",
    "    tf.nn.rnn_cell.DropoutWrapper(forward_cell,input_keep_prob=self.dropout_ph,output_keep_prob=self.dropout_ph,state_keep_prob=self.dropout_ph)\n",
    "    tf.nn.rnn_cell.DropoutWrapper(backward_cell,input_keep_prob=self.dropout_ph,output_keep_prob=self.dropout_ph,state_keep_prob=self.dropout_ph)\n",
    "\n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix_variable,self.input_batch)\n",
    "    \n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    (rnn_output_fw, rnn_output_bw), _ = tf.nn.bidirectional_dynamic_rnn(forward_cell,backward_cell,embeddings,sequence_length=self.lengths,dtype=tf.float32)\n",
    "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)\n",
    "    \n",
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "    \n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    softmax_output = tf.nn.softmax(self.logits,axis=None)\n",
    "    \n",
    "    # Use argmax (tf.argmax) to get the most probable tags\n",
    "    # Don't forget to set axis=-1\n",
    "    # otherwise argmax will be calculated in a wrong way\n",
    "    self.predictions = tf.argmax(softmax_output,axis=-1)\n",
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "    \n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits(_sentinel=None,labels=ground_truth_tags_one_hot,logits=self.logits,dim=-1,name=None)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    # Be careful that the argument of tf.reduce_mean should be\n",
    "    # multiplication of mask and loss_tensor.\n",
    "    self.loss = tf.reduce_mean(mask*loss_tensor,axis=None,keepdims=None,name=None,reduction_indices=None,keep_dims=None)\n",
    "\n",
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    self.optimizer =  tf.train.AdamOptimizer(learning_rate= self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars contains also variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad,clip_norm),var) for grad,var in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
    "\n",
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()\n",
    "\n",
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)\n",
    "\n",
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    \n",
    "    feed_dict = {self.input_batch: x_batch,self.lengths: lengths}\n",
    "    \n",
    "    predictions=session.run(self.predictions, feed_dict=feed_dict)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load (\"evaluation.py\")\n",
    "evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import precision_recall_f1\n",
    "\n",
    "def predict_tags(model, session, token_idxs_batch, lengths):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(tag_ind[tag_idx])\n",
    "            tokens.append(token_ind[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(tag_ind[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(vocabulary_size=20482,n_tags=21,embedding_dim=200,n_hidden_rnn=200,PAD_index=1)\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "learning_rate = 0.1\n",
    "learning_rate_decay = 1.414\n",
    "dropout_keep_probability = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 77920 phrases; correct: 188.\n",
      "\n",
      "precision:  0.24%; recall:  4.19%; F1:  0.46\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 9490 phrases; correct: 15.\n",
      "\n",
      "precision:  0.16%; recall:  2.79%; F1:  0.30\n",
      "\n",
      "-------------------- Epoch 2 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 3486 phrases; correct: 494.\n",
      "\n",
      "precision:  14.17%; recall:  11.00%; F1:  12.39\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 388 phrases; correct: 48.\n",
      "\n",
      "precision:  12.37%; recall:  8.94%; F1:  10.38\n",
      "\n",
      "-------------------- Epoch 3 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4882 phrases; correct: 785.\n",
      "\n",
      "precision:  16.08%; recall:  17.49%; F1:  16.75\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 400 phrases; correct: 69.\n",
      "\n",
      "precision:  17.25%; recall:  12.85%; F1:  14.73\n",
      "\n",
      "-------------------- Epoch 4 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 5608 phrases; correct: 1183.\n",
      "\n",
      "precision:  21.09%; recall:  26.35%; F1:  23.43\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 427 phrases; correct: 90.\n",
      "\n",
      "precision:  21.08%; recall:  16.76%; F1:  18.67\n",
      "\n",
      "-------------------- Epoch 5 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 5430 phrases; correct: 1647.\n",
      "\n",
      "precision:  30.33%; recall:  36.69%; F1:  33.21\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 404 phrases; correct: 121.\n",
      "\n",
      "precision:  29.95%; recall:  22.53%; F1:  25.72\n",
      "\n",
      "-------------------- Epoch 6 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 5388 phrases; correct: 1991.\n",
      "\n",
      "precision:  36.95%; recall:  44.35%; F1:  40.32\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 402 phrases; correct: 131.\n",
      "\n",
      "precision:  32.59%; recall:  24.39%; F1:  27.90\n",
      "\n",
      "-------------------- Epoch 7 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 5105 phrases; correct: 2464.\n",
      "\n",
      "precision:  48.27%; recall:  54.89%; F1:  51.37\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 375 phrases; correct: 133.\n",
      "\n",
      "precision:  35.47%; recall:  24.77%; F1:  29.17\n",
      "\n",
      "-------------------- Epoch 8 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 5230 phrases; correct: 2920.\n",
      "\n",
      "precision:  55.83%; recall:  65.05%; F1:  60.09\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 414 phrases; correct: 137.\n",
      "\n",
      "precision:  33.09%; recall:  25.51%; F1:  28.81\n",
      "\n",
      "-------------------- Epoch 9 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 5140 phrases; correct: 3346.\n",
      "\n",
      "precision:  65.10%; recall:  74.54%; F1:  69.50\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 437 phrases; correct: 148.\n",
      "\n",
      "precision:  33.87%; recall:  27.56%; F1:  30.39\n",
      "\n",
      "-------------------- Epoch 10 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4980 phrases; correct: 3640.\n",
      "\n",
      "precision:  73.09%; recall:  81.09%; F1:  76.88\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 429 phrases; correct: 152.\n",
      "\n",
      "precision:  35.43%; recall:  28.31%; F1:  31.47\n",
      "\n",
      "-------------------- Epoch 11 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4849 phrases; correct: 3847.\n",
      "\n",
      "precision:  79.34%; recall:  85.70%; F1:  82.39\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 430 phrases; correct: 153.\n",
      "\n",
      "precision:  35.58%; recall:  28.49%; F1:  31.64\n",
      "\n",
      "-------------------- Epoch 12 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4760 phrases; correct: 3970.\n",
      "\n",
      "precision:  83.40%; recall:  88.44%; F1:  85.85\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 421 phrases; correct: 156.\n",
      "\n",
      "precision:  37.05%; recall:  29.05%; F1:  32.57\n",
      "\n",
      "-------------------- Epoch 13 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4741 phrases; correct: 4038.\n",
      "\n",
      "precision:  85.17%; recall:  89.95%; F1:  87.50\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 421 phrases; correct: 156.\n",
      "\n",
      "precision:  37.05%; recall:  29.05%; F1:  32.57\n",
      "\n",
      "-------------------- Epoch 14 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4724 phrases; correct: 4077.\n",
      "\n",
      "precision:  86.30%; recall:  90.82%; F1:  88.51\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 422 phrases; correct: 157.\n",
      "\n",
      "precision:  37.20%; recall:  29.24%; F1:  32.74\n",
      "\n",
      "-------------------- Epoch 15 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4700 phrases; correct: 4083.\n",
      "\n",
      "precision:  86.87%; recall:  90.96%; F1:  88.87\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 414 phrases; correct: 156.\n",
      "\n",
      "precision:  37.68%; recall:  29.05%; F1:  32.81\n",
      "\n",
      "-------------------- Epoch 16 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4694 phrases; correct: 4090.\n",
      "\n",
      "precision:  87.13%; recall:  91.11%; F1:  89.08\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 412 phrases; correct: 156.\n",
      "\n",
      "precision:  37.86%; recall:  29.05%; F1:  32.88\n",
      "\n",
      "-------------------- Epoch 17 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4687 phrases; correct: 4103.\n",
      "\n",
      "precision:  87.54%; recall:  91.40%; F1:  89.43\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 410 phrases; correct: 156.\n",
      "\n",
      "precision:  38.05%; recall:  29.05%; F1:  32.95\n",
      "\n",
      "-------------------- Epoch 18 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4674 phrases; correct: 4098.\n",
      "\n",
      "precision:  87.68%; recall:  91.29%; F1:  89.45\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 405 phrases; correct: 156.\n",
      "\n",
      "precision:  38.52%; recall:  29.05%; F1:  33.12\n",
      "\n",
      "-------------------- Epoch 19 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4679 phrases; correct: 4116.\n",
      "\n",
      "precision:  87.97%; recall:  91.69%; F1:  89.79\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 406 phrases; correct: 156.\n",
      "\n",
      "precision:  38.42%; recall:  29.05%; F1:  33.09\n",
      "\n",
      "-------------------- Epoch 20 of 20 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4680 phrases; correct: 4125.\n",
      "\n",
      "precision:  88.14%; recall:  91.89%; F1:  89.98\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 406 phrases; correct: 156.\n",
      "\n",
      "precision:  38.42%; recall:  29.05%; F1:  33.09\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_word, train_tag, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_word, validation_tag, short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_word, train_tag):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 4683 phrases; correct: 4128.\n",
      "\n",
      "precision:  88.15%; recall:  91.96%; F1:  90.01\n",
      "\n",
      "\t     company: precision:   91.81%; recall:   94.09%; F1:   92.93; predicted:   659\n",
      "\n",
      "\t    facility: precision:   83.83%; recall:   89.17%; F1:   86.42; predicted:   334\n",
      "\n",
      "\t     geo-loc: precision:   95.55%; recall:   96.99%; F1:   96.26; predicted:  1011\n",
      "\n",
      "\t       movie: precision:   80.56%; recall:   85.29%; F1:   82.86; predicted:    72\n",
      "\n",
      "\t musicartist: precision:   85.54%; recall:   89.22%; F1:   87.34; predicted:   242\n",
      "\n",
      "\t       other: precision:   77.84%; recall:   86.79%; F1:   82.07; predicted:   844\n",
      "\n",
      "\t      person: precision:   92.65%; recall:   95.26%; F1:   93.93; predicted:   911\n",
      "\n",
      "\t     product: precision:   78.84%; recall:   85.53%; F1:   82.05; predicted:   345\n",
      "\n",
      "\t  sportsteam: precision:   91.79%; recall:   87.56%; F1:   89.62; predicted:   207\n",
      "\n",
      "\t      tvshow: precision:   84.48%; recall:   84.48%; F1:   84.48; predicted:    58\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 407 phrases; correct: 156.\n",
      "\n",
      "precision:  38.33%; recall:  29.05%; F1:  33.05\n",
      "\n",
      "\t     company: precision:   66.67%; recall:   48.08%; F1:   55.87; predicted:    75\n",
      "\n",
      "\t    facility: precision:   32.43%; recall:   35.29%; F1:   33.80; predicted:    37\n",
      "\n",
      "\t     geo-loc: precision:   56.32%; recall:   43.36%; F1:   49.00; predicted:    87\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     6\n",
      "\n",
      "\t musicartist: precision:    8.33%; recall:    3.57%; F1:    5.00; predicted:    12\n",
      "\n",
      "\t       other: precision:   25.58%; recall:   27.16%; F1:   26.35; predicted:    86\n",
      "\n",
      "\t      person: precision:   23.94%; recall:   15.18%; F1:   18.58; predicted:    71\n",
      "\n",
      "\t     product: precision:   12.00%; recall:    8.82%; F1:   10.17; predicted:    25\n",
      "\n",
      "\t  sportsteam: precision:   25.00%; recall:   10.00%; F1:   14.29; predicted:     8\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 475 phrases; correct: 191.\n",
      "\n",
      "precision:  40.21%; recall:  31.62%; F1:  35.40\n",
      "\n",
      "\t     company: precision:   58.93%; recall:   39.29%; F1:   47.14; predicted:    56\n",
      "\n",
      "\t    facility: precision:   38.64%; recall:   36.17%; F1:   37.36; predicted:    44\n",
      "\n",
      "\t     geo-loc: precision:   65.79%; recall:   45.45%; F1:   53.76; predicted:   114\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     7\n",
      "\n",
      "\t musicartist: precision:   12.50%; recall:    3.70%; F1:    5.71; predicted:     8\n",
      "\n",
      "\t       other: precision:   28.57%; recall:   31.07%; F1:   29.77; predicted:   112\n",
      "\n",
      "\t      person: precision:   31.03%; recall:   25.96%; F1:   28.27; predicted:    87\n",
      "\n",
      "\t     product: precision:    8.70%; recall:    7.14%; F1:    7.84; predicted:    23\n",
      "\n",
      "\t  sportsteam: precision:   19.05%; recall:   12.90%; F1:   15.38; predicted:    21\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_word, train_tag, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_word, validation_tag, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_word, test_tag, short_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
